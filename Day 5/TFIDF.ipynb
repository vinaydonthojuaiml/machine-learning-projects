{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f8163b8",
   "metadata": {},
   "source": [
    "# Stopwords Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "856ac33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'an', 'example', 'sentence', 'with', 'some', 'stopwords', 'that', 'we', 'want', 'to', 'remove', '.']\n",
      "example sentence stopwords want remove .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10061]\n",
      "[nltk_data]     No connection could be made because the target machine\n",
      "[nltk_data]     actively refused it>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    print(words)\n",
    "    # Get the English stopwords list from NLTK\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Remove stopwords from the tokenized words\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    # Join the filtered words back into a sentence\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    \n",
    "    return filtered_text\n",
    "\n",
    "# Example usage\n",
    "text = \"This is an example sentence with some stopwords that we want to remove.\"\n",
    "filtered_text = remove_stopwords(text)\n",
    "print(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e503944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample', 'sentence', 'contains', 'punctuation', 'stopwords', 'need', 'processed']\n",
      "['sampl', 'sentenc', 'contain', 'punctuat', 'stopword', 'need', 'process']\n",
      "Original Text:\n",
      "this is a sample sentence it contains punctuation and stopwords that need to be processed\n",
      "\n",
      "Preprocessed Text:\n",
      "sampl sentenc contain punctuat stopword need process\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "\n",
    "# Sample text data (replace with your dataset)\n",
    "sample_text = \"This is a sample sentence. It contains punctuation and stopwords that need to be processed.\"\n",
    "\n",
    "# Step 1: Lowercasing\n",
    "sample_text = sample_text.lower()\n",
    "\n",
    "# Step 2: Removing Punctuation\n",
    "sample_text = ''.join([char for char in sample_text if char not in string.punctuation])\n",
    "\n",
    "# Step 3: Tokenization\n",
    "tokens = word_tokenize(sample_text)\n",
    "\n",
    "# Step 4: Stopword Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "print(filtered_tokens)\n",
    "# Step 5: Stemming (You can use Lemmatization if needed)\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(stemmed_tokens)\n",
    "# Step 6: Join tokens back into a sentence\n",
    "preprocessed_text = ' '.join(stemmed_tokens)\n",
    "\n",
    "# Print the preprocessed text\n",
    "print(\"Original Text:\")\n",
    "print(sample_text)\n",
    "print(\"\\nPreprocessed Text:\")\n",
    "print(preprocessed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "759e3df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Values:\n",
      "        and  document     first        is       one    second       the  \\\n",
      "0  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
      "1  0.000000  0.687624  0.000000  0.281089  0.000000  0.538648  0.281089   \n",
      "2  0.511849  0.000000  0.000000  0.267104  0.511849  0.000000  0.267104   \n",
      "3  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
      "\n",
      "      third      this  \n",
      "0  0.000000  0.384085  \n",
      "1  0.000000  0.281089  \n",
      "2  0.511849  0.267104  \n",
      "3  0.000000  0.384085  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents (replace with your own dataset)\n",
    "documents = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\"\n",
    "]\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the feature names (words) in the TF-IDF matrix\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the TF-IDF matrix to a dense array for easier manipulation\n",
    "tfidf_matrix_array = tfidf_matrix.toarray()\n",
    "\n",
    "# Create a DataFrame (optional) for better visualization\n",
    "import pandas as pd\n",
    "tfidf_df = pd.DataFrame(data=tfidf_matrix_array, columns=feature_names)\n",
    "\n",
    "# Print the TF-IDF values\n",
    "print(\"TF-IDF Values:\")\n",
    "print(tfidf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afeb78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe136db6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951679b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb2d600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0e7273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ce6e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2afbe6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff2ad17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7dbffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaef85a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7af3008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecf60e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c2d97c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1c541e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d524eb53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89165369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff83ac38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7344a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
